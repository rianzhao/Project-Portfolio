{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fce1d5e5",
   "metadata": {},
   "source": [
    "> The Github Repo of this portfolio (as well as each assignment) is avalable at: https://github.com/rianzhao/Projects-Portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda6a8b-44a2-4f04-871f-73b89d5774b1",
   "metadata": {},
   "source": [
    "#### In this portfolio, I will explain the three projects I have done for Advanced Marchine Learning course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ba6c3-c06a-4e22-b985-f76a59c82233",
   "metadata": {},
   "source": [
    "### Project 1: U.N. world happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66b726-0714-4b51-9cf2-5cecc82a5f20",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset is stored in a zip file and has already been split into three parts: x_train, y_train, and x_test. The x_train part contains a set of variables that include 'Country or region, GDP per capita, Social support, Healthy life expectancy, Freedom to make life choices, Generosity, Perceptions of corruption, name, region, sub-region, and Terrorist_attacks'. The y_train part contains the happiness value of each observation in the training set, which is categorized into one of five categories: \"average, high, very high, low, very low\".\n",
    "The goal of the project is to build a predictive model that can accurately predict the happiness value of a country based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea3c88-8e7d-4a75-b965-6201395c09f2",
   "metadata": {},
   "source": [
    "#### Predictive Models\n",
    "##### RFC, GBC, Keras\n",
    "I began by building three simple models: a random forest classifier, a GradientBoosting classifier, and a Keras deep learning model. However, I found that the accuracy score of both the random forest and GradientBoosting classifiers was only 0.5, indicating that they were not performing significantly better than random chance. Additionally, the small size of the training dataset made it difficult to effectively train the Keras deep learning model.\n",
    "To improve the performance of my models, I tried tuning the hyperparameters using cross-validation. However, this did not significantly improve the performance of the random forest or Keras deep learning models. I did have some success with the GradientBoosting classifier, which achieved an accuracy score of 0.656481 on the training set after tuning its hyperparameters using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae816cd8-880b-4012-b5f3-d56e3a1d31be",
   "metadata": {},
   "source": [
    "#### Salient Details\n",
    "I encountered some difficulties when submitting my tuned GradientBoosting model for evaluation. Although I was able to improve its performance on the training set, the model scored exactly the same as the untuned model. I tried to troubleshoot this issue by re-writing my code, but was unable to find a solution.\n",
    "According to the accuracy score leaderboard. Most submissions were having the same difficulty of exceeding the \"limitation\" of accuracy score 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c6e89-0b21-46a8-8bdf-86be26d266ee",
   "metadata": {},
   "source": [
    "### Project 2: Covid Positive X-Ray image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050816bf-e9c2-4833-8299-629da067618d",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset used for this project was stored in a zip file and contained medical images of chest X-rays of patients that have been diagnosed with COVID-19 or have tested negative for the virus. \n",
    "The goal of the project was to build predictive models that could help identify whether a patient has COVID-19 or not, based on the features observed in their chest X-rays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8f6ec-0239-4904-b87d-bb54976637e3",
   "metadata": {},
   "source": [
    "#### Predictive Models\n",
    "##### 4-lays CNN with Modelcheckpoints\n",
    "The first model was a 4-layer CNN that was trained for 50 epochs to avoid underfitting, and ModelCheckpoint was used to save the model with the largest validation accuracy. This model was able to achieve an accuracy of 92.33% on the test set and was ranked 16th on the competition leaderboard.\n",
    "##### Transfer Learning with MobileNetV2\n",
    "The second model made use of transfer learning, based on a MobileNetV2 pre-trained on ImageNet, with the top layer removed and three fully-connected layers added to fine-tune on the dataset. This model showed fast convergence and achieved an accuracy of 94.89% on the test set, which was ranked 6th on the leaderboard.\n",
    "##### Ensemble of 4-lays CNN and MobileNetV2\n",
    "The third model was a simple ensemble of two existing models, which included the 4-layer CNN model and the fine-tuned MobileNetV2 model. A simple average was performed on the predicted values of these two models to predict the final output. This ensemble model achieved an accuracy of 96.05% on the leaderboard, which was ranked 3rd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73575227-d6c2-4a3f-bbe9-dbb4788ade76",
   "metadata": {},
   "source": [
    "#### Salient details \n",
    "I used simple 5-layer MLP to prove out dataset is not \"trival\". \n",
    "\n",
    "The results suggest that the ensemble model achieved the best performance on the dataset and outperformed both individual models. The use of transfer learning in the MobileNetV2 model also proved to be an effective strategy for achieving high accuracy with minimal training time. The project's performance on the competition leaderboard was highly competitive and suggests that the models developed here could have practical applications in medical diagnosis of COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dfdc7-2259-457e-ac03-a68581a8f1c7",
   "metadata": {},
   "source": [
    "### Project 3: Text_Classification_Using_the_Stanford_SST_Sentiment_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba907cd-337a-49d9-a4d5-3f583336d121",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset used for this project contains over 11,000 sentences from movie reviews labeled with sentiment score from very negative to very positive.\n",
    "\n",
    "Each sentence in the dataset has been assigned a binary sentiment label, indicating whether the sentence expresses a positive or negative sentiment. Additionally, each sentence has been annotated with a fine-grained sentiment score on a scale from 0 to 4, where 0 indicates very negative sentiment and 4 indicates very positive sentiment. The fine-grained sentiment score is intended to capture more nuanced differences in sentiment between sentences, beyond simply positive or negative.\n",
    "\n",
    "The goal of this project was to build predive model to predict whether a sentence is negative or positive emotional tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa14d7-c626-455a-9c89-73d53349210d",
   "metadata": {},
   "source": [
    "#### Preditive Models\n",
    "##### Embedding + LSTM\n",
    "The first model I implemented is an embedding layer followed by two LSTM layers and used the Adam optimizer and enabled early stopping and best model saving during the training. This model received a 68.61% accuracy, ranked 42 on the leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704eedd-fac7-47c7-8487-b9173f807a81",
   "metadata": {},
   "source": [
    "##### Embedding + CNN\n",
    "The second model I implemented is an embedding layer followed by a 1D CNN layer and a global max pooling layer. This model received a 72.89% accuracy, ranked 35th on the leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513b506-9f67-434e-9ec0-4eae6ec1b954",
   "metadata": {},
   "source": [
    "##### Transfer Learning with GloVE\n",
    "The third model I implemented is still an emebdding layer with an LSTM layer. But this time I use the pretrained GloVE word vector as the intialization for the embedding layer. Specifically, I use the 200-D pretrained vectors. This model received a 81.78% accuracy, ranked tied 1st on the leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e80f65-8088-46f2-95b2-facdd2e81222",
   "metadata": {},
   "source": [
    "#### Salient Details: \n",
    "I made 2 changes to the tokenizer before building the preditive models. \n",
    "1: I added a oov_token to handle the case when thea data has words that are not in the vocabulary.\n",
    "2: I increased the sequence max length from 40 to 64, because I found that the max length in the dataset is 52, using max length 40 will definitely harm the model performance.\n",
    "\n",
    "I was interested in determining the upper bound of deep learning models for sentiment analysis. So, I utilized a fine-tuned BERT model from HuggingFace's transformer library. The tokenizer and model I used are 'distilbert-base-uncased-finetunes-sst-2-english' and 'distilbert-base-uncased-finetuned-sst-2-english'. \n",
    "The model is relatively large, and we realized that fine-tuning it by adding a small dense layer would take a long time to train and could even harm its performance. Therefore, we decided to remove the additional layer and used the pretrained model for prediction directly.The results were a significant improvement, as we not only achieved the first place, but also got an accuracy of 92.32%, leading the leaderboard by a large margin of over 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7601ed-b44f-42d4-9aa6-dd0f00da5aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
